# Vector Embedding
Vector embedding is a technique used to represent words, phrases, or entire documents as vectors in a continuous vector space. This representation allows for the capture of semantic relationships between different pieces of text, enabling various natural language processing (NLP) tasks such as text classification, clustering, and recommendation systems.
### Example
similar words have similar vector representations. For instance, the words "king" and "queen" would have vectors that are close to each other in the vector space, reflecting their semantic similarity.

## Common Techniques for Vector Embedding
1. **Word2Vec**: A popular method that uses neural networks
 to learn word associations from a large corpus of text. It produces two types of embeddings: Continuous Bag of Words (CBOW) and Skip-gram.

## visual example of Word2Vec
[https://projector.tensorflow.org/](https://projector.tensorflow.org/)